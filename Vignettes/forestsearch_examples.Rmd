---
title: "forestsearch examples"
output: 
  html_document:
         code_folding: hide
---


# Introduction

## Purpose

This document provides a comprehensive test suite for the **optimized ForestSearch**
pipeline. The test demonstrates:

- Generation of realistic synthetic survival data with known subgroup structure
- Full ForestSearch analysis with GRF and LASSO
- Validation of results against ground truth
- Performance benchmarking
- Interpretation guidance

## What is ForestSearch?

ForestSearch is an R package for identifying patient subgroups with differential
treatment effects in survival data. It combines:

- **Machine Learning**: GRF (Generalized Random Forests) for variable selection
- **Statistical Methods**: Cox regression, LASSO dimension reduction
- **Validation**: Consistency analysis across random splits
- **Parallelization**: Efficient computation for large datasets

## Key Features
- **Subgroup Identification:** Finds subgroups with differential treatment effects using combinations of confounders.
- **Variable Selection:** Supports LASSO (for dimension reduction), and Generalized Random Forests (GRF) for candidate cut selection.
- **Flexible Cut Strategies:** Allows for forced cuts (e.g., "age <= 65", "biomarker < 1%", "biomarker < 2%", ..., "biomarker < 10%", etc), median cuts, and quantile cut strategies.
- **Parallelization:** Supports parallel processing for efficiency.
- **Consistency Analysis:** Evaluates the consistency of identified subgroups across splits or bootstraps.

## Main Arguments
- `df.analysis`: Data frame for analysis.
- `outcome.name`, `event.name`, `treat.name`, `id.name`: Variable names for outcome, event, treatment, and ID.
- `confounders.name`: Names of confounder variables.
- `parallel_args`: List for parallelization (plan, workers).
- `use_lasso`, `use_grf`: Logical; use LASSO or GRF for variable selection.
- `conf_force`: Allows for pre-specified subgroup (e.g., "age <= 65", "biomarker <= low").
- `n.min`, `hr.threshold`, `hr.consistency`, `sg_focus`, `fs.splits`, `stop.threshold`, `pconsistency.threshold`: Subgroup search and consistency parameters.
- `details`: Logical; print details during execution.

# ForestSearch Parameter Guide: hr.threshold, hr.consistency, and pconsistency.threshold

## Overview
These three parameters control the **effect size requirements** and **reproducibility standards** for subgroup identification. They act as filters to ensure identified subgroups show both clinically meaningful and statistically stable treatment effects.

## Parameter Definitions

### `hr.threshold` - Initial Hazard Ratio Threshold
- **Default**: 1.25
- **Purpose**: Minimum treatment effect required for initial subgroup consideration
- **Range**: 1.0 (any effect) to 2.0+ (large effects only)
- **Interpretation**: HR > 1.25 means treatment is at least 25% worse in this subgroup

### `hr.consistency` - Consistency Hazard Ratio Threshold
- **Default**: 1.0
- **Purpose**: Minimum HR required in random split-sample validation
- **Range**: 0.8-1.0 (lenient) to 1.25+ (strict)
- **Interpretation**: Ensures treatment effect persists in validation splits

### `pconsistency.threshold` - Proportion Consistency Threshold
- **Default**: 0.90 (90%)
- **Purpose**: Minimum proportion of random splits that must meet hr.consistency
- **Range**: 0.5 (very lenient) to 0.95+ (very strict)
- **Interpretation**: Out of 1000 splits, at least 900 must show HR > hr.consistency



## The Complete Validation Flow
```{r flowchartA, fig.cap="Figure 1: Overview of forestsearch steps", label="fig:flowA", echo = FALSE}
library(DiagrammeR)

# Create the flowchart using grViz
flow1 <- grViz("
digraph validation_flow {
  # Graph settings
  graph [layout = dot, rankdir = TB, fontsize = 12]
  
  # Node settings
  node [shape = rectangle, style = filled, fillcolor = lightblue, 
        fontname = Helvetica, fontsize = 10]
  
  # Define nodes
  start [label = 'Candidate Subgroup', shape = ellipse, fillcolor = pink]
  check_hr [label = 'HR > hr.threshold?', shape = diamond, fillcolor = lightyellow]
  reject1 [label = 'Rejected\\n(Effect too small)', fillcolor = salmon]
  splits [label = '1000 Random\\n50/50 Splits', fillcolor = lightcyan]
  check_splits [label = 'Both splits:\\nHR > hr.consistency?', shape = diamond, fillcolor = lightyellow]
  count [label = 'Count Consistent\\nSplits', fillcolor = lightcyan]
  check_prop [label = 'Proportion ≥\\npconsistency.threshold?', shape = diamond, fillcolor = lightyellow]
  reject2 [label = 'Rejected\\n(Unstable effect)', fillcolor = salmon]
  accept [label = 'Selected\\n(Stable subgroup)', fillcolor = pink, shape = ellipse]
  
  # Define edges
  start -> check_hr
  check_hr -> reject1 [label = 'No']
  check_hr -> splits [label = 'Yes']
  splits -> check_splits
  check_splits -> count [label = 'Evaluate\\neach split']
  count -> check_prop
  check_prop -> reject2 [label = 'No']
  check_prop -> accept [label = 'Yes']
  
  # Add parameter annotations
  {rank = same; check_hr; param1 [label = 'Parameter:\\nhr.threshold', 
                                    shape = note, fillcolor = white]}
  {rank = same; check_splits; param2 [label = 'Parameter:\\nhr.consistency', 
                                       shape = note, fillcolor = white]}
  {rank = same; check_prop; param3 [label = 'Parameter:\\npconsistency.threshold', 
                                     shape = note, fillcolor = white]}
  
  # Connect annotations with dotted lines
  param1 -> check_hr [style = dotted, arrowhead = none, color = lightgrey]
  param2 -> check_splits [style = dotted, arrowhead = none, color = lightgrey]
  param3 -> check_prop [style = dotted, arrowhead = none, color = lightgrey]
}
", width =800, height = 400)

flow1

```



# ForestSearch Parameter Guide: n.min, d0.min, and d1.min

## Overview
These three parameters are **critical safety thresholds** that ensure identified subgroups have sufficient statistical reliability and prevent spurious findings from small or event-sparse subgroups.

## Parameter Definitions

### `n.min` - Minimum Subgroup Size
- **Default**: 60 patients
- **Purpose**: Ensures statistical stability and prevents overfitting
- **Range**: 30-50 (exploratory) to 100+ (confirmatory)
- **Rule of thumb**: At least 5-10% of total sample size

### `d0.min` - Minimum Events in Control Group
- **Default**: 10 events
- **Purpose**: Ensures reliable baseline risk estimation
- **Range**: 5 (minimum) to 20+ (recommended)
- **Critical**: Below 5 events, Cox models become unstable

### `d1.min` - Minimum Events in Treatment Group
- **Default**: 10 events
- **Purpose**: Ensures reliable treatment effect estimation
- **Range**: 5 (minimum) to 20+ (recommended)
- **Critical**: Below 5 events, hazard ratios become unreliable


```{r flowchartB, fig.cap="Figure 1: Overview of forestsearch steps", label="fig:flowB", echo = FALSE, eval = FALSE}
# More detailed flowchart with example values
flow2 <- grViz("
digraph detailed_validation {
  # Graph settings
  graph [layout = dot, rankdir = TB, fontsize = 14, 
         label = 'ForestSearch Subgroup Validation Pipeline', 
         labelloc = t]
  
  # Node settings  
  node [shape = box, style = 'filled,rounded', fillcolor = white, 
        fontname = Arial, fontsize = 11, penwidth = 2]
  
  # Stage 1: Discovery
  subgraph cluster_0 {
    label = 'Stage 1: Discovery'
    style = 'dashed,rounded'
    color = darkgreen
    fontsize = 12
    
    input [label = 'Subgroup Found\\nHR = 1.45\\nn = 120 patients\\nevents = 35', 
           fillcolor = '#E8F5E9', color = darkgreen]
    test1 [label = '1.45 > 1.25?\\n(hr.threshold)', 
           shape = diamond, fillcolor = '#FFF3E0', color = orange]
    pass1 [label = 'Proceed to\\nValidation', fillcolor = '#E8F5E9', color = darkgreen]
  }
  
  # Stage 2: Validation
  subgraph cluster_1 {
    label = 'Stage 2: Split-Sample Validation'
    style = 'dashed,rounded'
    color = darkblue
    fontsize = 12
    
    split [label = 'Random Split #i\\n(i = 1 to 1000)\\n60 patients each half', 
           fillcolor = '#E3F2FD', color = darkblue]
    calc [label = 'Calculate:\\nHR₁ = 1.38\\nHR₂ = 1.42', 
          fillcolor = '#E3F2FD', color = darkblue]
    test2 [label = '1.38 > 1.0?\\n1.42 > 1.0?\\n(hr.consistency)', 
           shape = diamond, fillcolor = '#FFF3E0', color = orange]
    success [label = 'Split i: ✓\\nConsistent', fillcolor = '#E8F5E9', color = darkgreen]
    fail [label = 'Split i: ✗\\nInconsistent', fillcolor = '#FFEBEE', color = darkred]
  }
  
  # Stage 3: Confirmation
  subgraph cluster_2 {
    label = 'Stage 3: Final Selection'
    style = 'dashed,rounded'
    color = purple
    fontsize = 12
    
    tally [label = 'Final Tally:\\n920/1000 consistent\\n(92% success rate)', 
           fillcolor = '#F3E5F5', color = purple]
    test3 [label = '0.92 ≥ 0.90?\\n(pconsistency.threshold)', 
           shape = diamond, fillcolor = '#FFF3E0', color = orange]
    selected [label = 'SELECTED\\nStable Subgroup\\nReport: HR=1.45', 
              fillcolor = '#C8E6C9', color = darkgreen, penwidth = 3]
    rejected [label = 'REJECTED\\nUnstable Effect', 
              fillcolor = '#FFCDD2', color = darkred]
  }
  
  # Edges
  input -> test1
  test1 -> pass1 [label = 'Yes', color = darkgreen]
  test1 -> rejected [label = 'No', color = darkred]
  pass1 -> split
  split -> calc
  calc -> test2
  test2 -> success [label = 'Both Yes', color = darkgreen]
  test2 -> fail [label = 'Any No', color = darkred]
  success -> tally
  fail -> tally
  tally -> test3
  test3 -> selected [label = 'Yes', color = darkgreen, penwidth = 2]
  test3 -> rejected [label = 'No', color = darkred]
  
  # # Add legend
  # subgraph cluster_legend {
  #   label = 'Parameter Effects'
  #   style = filled
  #   fillcolor = '#FAFAFA'
  #   fontsize = 10
  #   
  #   legend1 [shape = none, fillcolor = none,
  #            label = 'hr.threshold: Higher = Fewer candidates']
  #   legend2 [shape = none, fillcolor = none,
  #            label = 'hr.consistency: Higher = Stricter validation']  
  #   legend3 [shape = none, fillcolor = none,
  #            label = 'pconsistency.threshold: Higher = More confidence']
  # }
}
", width =800, height = 400)

flow2

```


```{r setup, include=FALSE}
# Load required libraries
library(forestsearch)
library(knitr)
library(DT)
library(DiagrammeR)
```

# Overview of sg_focus Parameter

The `sg_focus` parameter determines **which subgroup to prioritize** when multiple candidates meet the consistency criteria. It represents different clinical and statistical objectives for identifying treatment harm subgroups.

## Quick Reference Table
```{r sg-focus-table, echo=FALSE}
sg_focus_summary <- data.frame(
  Option = c("hr", "maxSG", "minSG", "hrMaxSG", "hrMinSG"),
  `Statistical Goal` = c(
    "Most reliable harmful effect",
    "Largest subgroup with harm",
    "Most specific subgroup with harm",
    "Largest subgroup with meaningful harm",
    "Most specific subgroup with meaningful harm"
  ),
  `Clinical Question` = c(
    "What harm are we most sure about?",
    "How many patients could we protect?",
    "Who is at highest risk?",
    "What's the biggest problem we're confident about?",
    "What's the most specific serious risk?"
  ),
  `Typical Use Case` = c(
    "Regulatory submissions",
    "Population health decisions",
    "Precision medicine",
    "Benefit-risk balance",
    "Biomarker development"
  ),
  stringsAsFactors = FALSE
)

kable(sg_focus_summary, 
      col.names = c("Option", "Statistical Goal", "Clinical Question", "Typical Use Case"),
      caption = "Summary of sg_focus Options")
```

# Detailed Description of Each Option

## sg_focus = "hr" (Default) {.tabset}

### Statistical Goal
Identify the subgroup with the **most reliable harmful effect**
  
  ### Selection Priority
  1. Highest consistency rate (Pcons)
2. Largest hazard ratio (HR)
3. Smallest number of factors (K)

### When to Use
- You want the most **statistically robust** finding
- Regulatory submissions requiring high confidence
- Primary goal is identifying clear treatment harm

### Example Code
```{r hr-example, eval=FALSE}
# Goal: FDA submission for drug approval
# Need: Most reliable evidence of harm
result <- forestsearch(
  df.analysis = cvd_trial,
  sg_focus = "hr",              # Statistical reliability
  hr.threshold = 1.3,           # Clinically meaningful
  pconsistency.threshold = 0.95 # Very high confidence
)

# Example interpretation of results:
# Two candidate subgroups found:
# Subgroup A: HR=1.8, Pcons=0.95, n=80  (elderly + diabetic)
# Subgroup B: HR=2.2, Pcons=0.85, n=45  (smokers)
# With sg_focus="hr", selects Subgroup A (better consistency)
```

## sg_focus = "maxSG" {.tabset}

### Statistical Goal
Identify the **largest subgroup** with any consistent harmful effect

### Selection Priority
1. Largest sample size (N)
2. Highest consistency rate (Pcons)
3. Smallest number of factors (K)

### When to Use
- You want to **protect the most patients** from harm
- Population health perspective
- Implementing broad safety restrictions

### Example Code
```{r maxsg-example, eval=FALSE}
# Goal: Treatment guidelines for health system
# Need: Protect maximum patients from harm
result <- forestsearch(
  df.analysis = population_data,
  sg_focus = "maxSG",           # Largest affected group
  hr.threshold = 1.2,           # Lower threshold acceptable
  pconsistency.threshold = 0.85 # Moderate confidence OK
)

# Example results:
# Subgroup A: HR=1.3, n=200, Pcons=0.91 (age > 65)
# Subgroup B: HR=1.8, n=80,  Pcons=0.93 (age > 65 + diabetic)
# Subgroup C: HR=2.5, n=30,  Pcons=0.95 (age > 65 + diabetic + CKD)
# With sg_focus="maxSG", selects Subgroup A (affects 200 patients)
```

## sg_focus = "minSG" {.tabset}

### Statistical Goal
Identify the **smallest, most specific subgroup** with consistent harm

### Selection Priority
1. Smallest sample size (N)
2. Highest consistency rate (Pcons)
3. Smallest number of factors (K)

### When to Use
- You want the most **targeted/specific** risk group
- Minimizing treatment restrictions
- Precision medicine applications

### Example Code
```{r minsg-example, eval=FALSE}
# Goal: Precision medicine approach
# Need: Most specific high-risk profile
result <- forestsearch(
  df.analysis = clinical_trial,
  sg_focus = "minSG",           # Most specific group
  hr.threshold = 1.5,           # High threshold for specificity
  pconsistency.threshold = 0.85 # Moderate confidence acceptable
)

# Using same example subgroups:
# With sg_focus="minSG", selects Subgroup C
# (Most specific: only 30 patients with all 3 risk factors)
```

## sg_focus = "hrMaxSG" {.tabset}

### Statistical Goal
Among subgroups with **meaningful harm** (HR > threshold), find the **largest**
  
  ### Two-Stage Selection
  1. First filter: Only subgroups with clinically important harm
2. Then select: The largest subgroup from filtered set

### When to Use
- Balance between **effect size and impact**
  - Want to protect many patients BUT only if harm is substantial
- Regulatory decisions with effect size thresholds

### Example Code
```{r hrmaxsg-example, eval=FALSE}
# Goal: Balance clinical significance with population impact
result <- forestsearch(
  df.analysis = trial_data,
  sg_focus = "hrMaxSG",         # Large group with meaningful harm
  hr.threshold = 1.5,           # Meaningful harm threshold
  pconsistency.threshold = 0.90 # High confidence
)

# Example with HR threshold = 1.5:
# Subgroup A: HR=1.3, n=200 (excluded: HR < 1.5)
# Subgroup B: HR=1.8, n=80  (included)
# Subgroup C: HR=2.5, n=30  (included)
# Selects Subgroup B (largest among those with HR > 1.5)
```

## sg_focus = "hrMinSG" {.tabset}

### Statistical Goal
Among subgroups with **meaningful harm**, find the **most specific**
  
  ### Two-Stage Selection
  1. First filter: Only subgroups with clinically important harm
2. Then select: The smallest/most specific from filtered set

### When to Use
- Identify **highest-risk patients** for special monitoring
- Design targeted safety protocols
- Companion diagnostic development

### Example Code
```{r hrminsg-example, eval=FALSE}
# Goal: Identify specific biomarker profile for treatment exclusion
result <- forestsearch(
  df.analysis = cancer_trial,
  sg_focus = "hrMinSG",         # Specific + harmful
  hr.threshold = 1.5,           # High harm threshold
  pconsistency.threshold = 0.90 # High confidence
)

# With HR threshold = 1.5:
# Selects Subgroup C (smallest among those with HR > 1.5)
# Most specific profile of patients with substantial harm
```



## Key Steps in the Function
1. **Input Validation:** Checks for required columns and arguments.
2. **GRF Variable Selection:** If enabled, runs GRF to identify important variables and cut points.
3. **Feature Selection:** Optionally applies LASSO for dimension reduction.
4. **Subgroup Search:** Searches for subgroups using combinations of selected variables.
5. **Consistency Analysis:** Evaluates the stability and consistency of identified subgroups.
6. **Prediction Dataset:** Optionally generates a prediction dataset with recommended treatment flags.
7. **Output:** Returns a list with subgroup definitions, candidate/evaluated confounders, prediction datasets, GRF results, and consistency metrics.

## Output
A list containing:
- `grp.consistency`: Subgroup consistency results.
- `find.grps`: Subgroup search results.
- `confounders.candidate`: Candidate confounders.
- `confounders.evaluated`: Evaluated confounders.
- `df.est`, `df.predict`, `df.test`: Datasets with subgroup flags.
- `minutes_all`: Total minutes elapsed.
- `grf_res`: GRF results.
- `sg_focus`, `sg.harm`, `grf_cuts`, `prop_maxk`, `max_sg_est`, `grf_plot`, `args_call_all`: Various results and metadata.



```{r flowchart1, fig.cap="Figure 1: Overview of forestsearch steps", label="fig:flow1", echo = FALSE, eval = FALSE}
library(DiagrammeR)

flowchart <- grViz("
digraph flowchart {
  node [shape = box, style = filled, fillcolor = lightblue]
  A [label = 'Input Data: df.analysis']
  B [label = 'Variable Selection']
  C [label = 'Feature Engineering']
  D [label = 'Subgroup Search']
  E [label = 'Consistency Analysis']
  F [label = 'Output Results']
  F1 [label = 'Subgroup Definitions']
  F2 [label = 'Candidate/Evaluated Confounders']
  F3 [label = 'Estimation/Prediction/Test Datasets']
  F4 [label = 'GRF Results & Plots']
  F5 [label = 'Summary Metrics']

  A -> B
  B -> C [label = 'LASSO/GRF/Forced Cuts']
  C -> D
  D -> E
  E -> F
  F -> F1
  F -> F2
  F -> F3
  F -> F4
  F -> F5
}
", width =800, height = 400)
flowchart
```



```{r}
rm(list=ls())

#library(devtools)
#install_github("larry-leon/forestsearch", force = TRUE)

suppressMessages(library(weightedSurv))
suppressMessages(library(forestsearch))
```

# GBSG data analysis example

---- Data Preparation ----
Prepare GBSG data
```{r}

library(survival)
df_gbsg <- gbsg
df_gbsg$tte <- df_gbsg$rfstime / 30.4375
df_gbsg$event <- df_gbsg$status
df_gbsg$treat <- df_gbsg$hormon
df_gbsg$grade3 <- ifelse(df_gbsg$grade == "3", 1, 0)
# create id name
df_gbsg$id <- seq_len(nrow(df_gbsg))
# If missing, then created automatically

tte.name <- "tte"
event.name <- "event"
treat.name <- "treat"
id.name <- "id"
arms <- c("treat", "control")

```


GBSG - ITT Analysis
```{r, fig.width = 10, fig.height=6}

dfcount_gbsg <- df_counting(df_gbsg, tte.name, event.name, treat.name, by.risk = 12)

plot_weighted_km(dfcount_gbsg, conf.int = TRUE, show.logrank = TRUE, 
                 put.legend.lr = "topleft", ymax = 1.05, xmed.fraction = 0.65)
title(main="GBSG trial")

```

## Subgroup identification analysis

```{r, fig.width = 10, fig.height = 8}

# Baseline factors from which candidate subgroups are formed
confounders.name<-c("age","meno","size","grade3","nodes","pgr","er")

library(doFuture)
library(doRNG)
registerDoFuture()
registerDoRNG()

# conf_force 'forces' a specific covariate cut, eg. 'age <= 65'


system.time({fs <- forestsearch(df_gbsg,  confounders.name=confounders.name,
                   outcome.name = "tte", treat.name = "treat", event.name = "event", id.name = "id",
                   hr.threshold = 1.25, hr.consistency = 1.0, pconsistency.threshold = 0.90,
                   sg_focus = "hrMaxSG",
                   showten_subgroups = FALSE, details=TRUE,
                   conf_force = c("age <= 65"),
                   cut_type = "default", use_grf = TRUE, plot.grf = TRUE, use_lasso = TRUE,
                   maxk = 2, n.min = 60, d0.min = 10, d1.min = 10,
                   plot.sg = TRUE, by.risk = 12,
                   parallel_args = list(plan="multisession", workers = 12, show_message = TRUE)
                   )
})

```

## Show (up to) top 10 subgroups

```{r}
res_tabs <- sg_tables(fs, ndecimals = 3)
res_tabs$sg10_out
```

## Un-adjusted summary estimates

```{r}
res_tabs$tab_estimates
```


## Bootstrap bias-correction and 95% CI estimation

```{r, eval = TRUE}
# Just run a few for illustration 
# In practice, recommend minimum of NB = 300 to 400 re-samples
# NOTE: workers = 1 and reset_parallel_fs = FALSE will use parallel process per the above forestsearch (fs) data analysis call 
# where workers are set to 12 which is the maximum on my machine;
# Also, note that if workers is set to a higher value than the max cores, this will be re-set to the maximum (max cores)

output_dir <- "results/"
save_results <- dir.exists(output_dir)

NB <- 30

t.start <- proc.time()[3]

fs_bc <- forestsearch_bootstrap_dofuture(fs.est = fs, nb_boots = NB, show_three = TRUE, details = TRUE)


t.now<-proc.time()[3]
t.min<-(t.now-t.start)/60
cat("Minutes (total) for bootstrap (boots,mins)",c(NB,t.min),"\n")
cat("Projected minutes for 1000",c(t.min*(1000/NB)),"\n")


if (save_results) {
    filename <- file.path(output_dir, 
                         paste0("bootstrap_results_B=", 
                                format(NB), 
                                ".RData"))
    save(fs_bc, fs, file = filename)
    cat("\nResults saved to:", filename, "\n")
  }

```

## Decision Function
```{r decision-function}
#' Decide sg_focus Based on Study Goal
#'
#' @param study_goal Character string indicating the primary study goal
#' @return Recommended sg_focus setting
#' @examples
#' decide_sg_focus("regulatory_safety")
#' decide_sg_focus("population_health")

decide_sg_focus <- function(study_goal) {
  
  decision_map <- list(
    "regulatory_safety" = "hr",        # Most reliable evidence
    "population_health" = "maxSG",     # Protect most patients
    "precision_medicine" = "minSG",    # Most targeted restriction
    "benefit_risk_balance" = "hrMaxSG", # Large group with meaningful harm
    "biomarker_development" = "hrMinSG" # Specific high-risk signature
  )
  
  if (!study_goal %in% names(decision_map)) {
    stop("Unknown study goal. Choose from: ", 
         paste(names(decision_map), collapse = ", "))
  }
  
  sg_focus <- decision_map[[study_goal]]
  
  cat("Study Goal:", study_goal, "\n")
  cat("Recommended sg_focus:", sg_focus, "\n")
  
  return(sg_focus)
}

# Example usage
decide_sg_focus("regulatory_safety")
```

# Impact on Algorithm Behavior

## Internal Sorting Logic
```{r sorting-logic, eval=FALSE}
# From subgroup_consistency.R
sort_subgroups <- function(result_new, sg_focus) {
  if (sg_focus == "hr") 
    data.table::setorder(result_new, -Pcons, -hr, K)  # Reliability first
  
  if (sg_focus %in% c("hrMaxSG", "maxSG")) 
    data.table::setorder(result_new, -N, -Pcons, K)   # Size first
  
  if (sg_focus %in% c("hrMinSG", "minSG")) 
    data.table::setorder(result_new, N, -Pcons, K)    # Specificity first
  
  result_new
}
```

## Parameter Interactions
```{r parameter-interactions, echo=FALSE}
param_interact <- data.frame(
  sg_focus = c("hr", "maxSG", "minSG", "hrMaxSG", "hrMinSG"),
  Typical_hr_threshold = c("1.25 (moderate)", "1.10 (low)", "1.50 (high)", 
                           "1.30 (moderate)", "1.50 (high)"),
  Typical_pconsistency = c("0.90 (high)", "0.85 (moderate)", "0.85 (moderate)",
                           "0.90 (high)", "0.90 (high)"),
  stop_threshold = c("0.90", "Variable*", "Variable*", "1.00", "1.00"),
  stringsAsFactors = FALSE
)

kable(param_interact,
      caption = "Typical Parameter Settings by sg_focus",
      col.names = c("sg_focus", "Typical hr.threshold", 
                    "Typical pconsistency.threshold", "stop.threshold"))
```

*Variable stop.threshold for size-based focus requires special handling based on the specific analysis goals.

# Practical Examples by Study Type

## Phase II Safety Study
```{r phase2-example, eval=FALSE}
# Option 1: Find most reliable signal
result_reliable <- forestsearch(
  df.analysis = phase2_data,
  sg_focus = "hr",              # Most reliable signal
  hr.threshold = 1.25,
  pconsistency.threshold = 0.85,
  n.min = 30,                   # Smaller sample OK for phase II
  d0.min = 5,
  d1.min = 5
)

# Option 2: Find highest-risk profile
result_highrisk <- forestsearch(
  df.analysis = phase2_data,
  sg_focus = "minSG",           # Most specific risk
  hr.threshold = 1.5,           # Higher threshold
  pconsistency.threshold = 0.80,
  n.min = 20,                   # Very specific groups OK
  d0.min = 5,
  d1.min = 5
)
```

## Phase III Confirmatory Trial
```{r phase3-example, eval=FALSE}
# Option 1: Regulatory standard
result_regulatory <- forestsearch(
  df.analysis = phase3_data,
  sg_focus = "hr",              # Regulatory standard
  hr.threshold = 1.3,
  pconsistency.threshold = 0.95, # Very high confidence
  n.min = 100,                  # Large samples required
  d0.min = 20,
  d1.min = 20
)

# Option 2: If population impact matters
result_population <- forestsearch(
  df.analysis = phase3_data,
  sg_focus = "hrMaxSG",         # Balance effect and size
  hr.threshold = 1.3,
  hr.consistency = 1.15,        # Must maintain effect
  pconsistency.threshold = 0.90,
  n.min = 80
)
```

## Post-Market Surveillance
```{r postmarket-example, eval=FALSE}
# Population protection priority
result_surveillance <- forestsearch(
  df.analysis = realworld_data,
  sg_focus = "maxSG",           # Protect most patients
  hr.threshold = 1.15,          # Lower threshold for safety
  pconsistency.threshold = 0.85,
  n.min = 100,
  d0.min = 15,
  d1.min = 15
)
```

## Biomarker Development
```{r biomarker-example, eval=FALSE}
# Specific signature with large effect
result_biomarker <- forestsearch(
  df.analysis = biomarker_trial,
  sg_focus = "hrMinSG",         # Specific + harmful
  hr.threshold = 2.0,           # Large effect required
  hr.consistency = 1.5,         # Must maintain large effect
  pconsistency.threshold = 0.90,
  n.min = 30,                   # Small specific groups OK
  d0.min = 10,
  d1.min = 10
)
```

# Accessing Results

## Understanding Output Structure
```{r output-structure, eval=FALSE}
# The grp.consistency object contains results for all approaches
fs_result <- forestsearch(df.analysis = trial_data, sg_focus = "hr")

# Access different sorting results
hr_results <- fs_result$grp.consistency$out_hr      # Sorted by HR priority
maxSG_results <- fs_result$grp.consistency$out_maxSG # Sorted by size (large)
minSG_results <- fs_result$grp.consistency$out_minSG # Sorted by size (small)

# The final selection depends on sg_focus
final_subgroup <- fs_result$sg.harm  # This is determined by sg_focus

# View top 10 candidates by different criteria
if (!is.null(hr_results)) {
  print(hr_results$result[1:min(10, nrow(hr_results$result)), ])
}
```

## Comparing Different sg_focus Results
```{r compare-results, eval=FALSE}
# Run same analysis with different sg_focus settings
compare_sg_focus <- function(df, ...) {
  
  focus_options <- c("hr", "maxSG", "minSG", "hrMaxSG", "hrMinSG")
  results <- list()
  
  for (focus in focus_options) {
    cat("\nRunning with sg_focus =", focus, "\n")
    
    res <- forestsearch(
      df.analysis = df,
      sg_focus = focus,
      details = FALSE,
      ...
    )
    
    if (!is.null(res$sg.harm)) {
      results[[focus]] <- list(
        subgroup = res$sg.harm,
        size = sum(res$df.est$treat.recommend == 0),
        consistency = res$grp.consistency$out_hr$result$Pcons[1]
      )
    } else {
      results[[focus]] <- "No subgroup found"
    }
  }
  
  return(results)
}

# Example usage
comparison <- compare_sg_focus(
  df = trial_data,
  hr.threshold = 1.25,
  pconsistency.threshold = 0.85,
  n.min = 50
)
```

# Statistical Considerations

## Power Trade-offs
```{r power-tradeoffs, echo=FALSE}
power_comparison <- data.frame(
  sg_focus = c("maxSG", "minSG", "hr"),
  Statistical_Power = c("Higher (more patients)", 
                        "Lower (fewer patients)", 
                        "Balanced"),
  Effect_Size = c("Potentially diluted", 
                  "Potentially stronger", 
                  "Based on consistency"),
  Clinical_Impact = c("Broad restrictions", 
                      "Targeted restrictions", 
                      "Evidence-based restrictions"),
  stringsAsFactors = FALSE
)

kable(power_comparison,
      caption = "Power and Impact Trade-offs by sg_focus")
```

## Implicit Utility Functions
```{r utility-functions}
# Utility function implicit in each focus

utility_hr <- function(subgroup) {
  # Maximize: consistency * effect_size
  subgroup$Pcons * subgroup$hr
}

utility_maxSG <- function(subgroup) {
  # Maximize: affected_population * (effect > threshold)
  subgroup$N * (subgroup$hr > hr.threshold)
}

utility_minSG <- function(subgroup) {
  # Maximize: specificity * effect_size
  (1/subgroup$N) * subgroup$hr
}

utility_hrMaxSG <- function(subgroup, hr.threshold) {
  # Maximize: size | hr > threshold
  ifelse(subgroup$hr > hr.threshold, subgroup$N, 0)
}

utility_hrMinSG <- function(subgroup, hr.threshold) {
  # Maximize: specificity | hr > threshold  
  ifelse(subgroup$hr > hr.threshold, 1/subgroup$N * subgroup$hr, 0)
}
```

# Sensitivity Analysis

## Testing Multiple sg_focus Settings
```{r sensitivity-analysis, eval=FALSE}
# Sensitivity analysis across sg_focus options
run_sg_sensitivity <- function(df, base_params) {
  
  focus_grid <- c("hr", "maxSG", "minSG", "hrMaxSG", "hrMinSG")
  
  results <- data.frame(
    sg_focus = character(),
    subgroup_found = logical(),
    subgroup_size = integer(),
    subgroup_hr = numeric(),
    consistency = numeric(),
    n_factors = integer(),
    stringsAsFactors = FALSE
  )
  
  for (focus in focus_grid) {
    res <- do.call(forestsearch, c(
      list(df.analysis = df, sg_focus = focus),
      base_params
    ))
    
    if (!is.null(res$sg.harm)) {
      results <- rbind(results, data.frame(
        sg_focus = focus,
        subgroup_found = TRUE,
        subgroup_size = sum(res$df.est$treat.recommend == 0),
        subgroup_hr = res$grp.consistency$out_hr$result$hr[1],
        consistency = res$grp.consistency$out_hr$result$Pcons[1],
        n_factors = length(res$sg.harm),
        stringsAsFactors = FALSE
      ))
    } else {
      results <- rbind(results, data.frame(
        sg_focus = focus,
        subgroup_found = FALSE,
        subgroup_size = NA,
        subgroup_hr = NA,
        consistency = NA,
        n_factors = NA,
        stringsAsFactors = FALSE
      ))
    }
  }
  
  return(results)
}

# Example usage
base_params <- list(
  hr.threshold = 1.25,
  pconsistency.threshold = 0.85,
  n.min = 50,
  d0.min = 10,
  d1.min = 10
)

sensitivity_results <- run_sg_sensitivity(trial_data, base_params)
print(sensitivity_results)
```

# Key Insights and Recommendations

## Philosophy of Each Approach
```{r philosophy-table, echo=FALSE}
philosophy <- data.frame(
  sg_focus = c("hr", "maxSG", "minSG", "hrMaxSG", "hrMinSG"),
  Core_Question = c(
    "What harm are we most sure about?",
    "How many patients could we protect?",
    "Who is at highest risk?",
    "What's the biggest problem we're confident about?",
    "What's the most specific serious risk?"
  ),
  Statistical_Philosophy = c(
    "Maximize reliability",
    "Maximize coverage",
    "Maximize specificity",
    "Balance impact and confidence",
    "Precision with significance"
  ),
  Clinical_Philosophy = c(
    "Evidence-based medicine",
    "Population health",
    "Precision medicine",
    "Risk-benefit optimization",
    "Targeted intervention"
  ),
  stringsAsFactors = FALSE
)

DT::datatable(philosophy,
              caption = "Philosophy Behind Each sg_focus Option",
              options = list(pageLength = 5, dom = 't'))
```

## Summary Recommendations

1. **Start with `sg_focus = "hr"`** for initial exploration - most balanced approach
2. **Consider your stakeholders**: 
  - Regulators → "hr"
- Health systems → "maxSG"
- Precision medicine → "minSG" or "hrMinSG"
3. **Run sensitivity analyses** with multiple sg_focus settings
4. **Document your choice** and rationale in study protocols
5. **Report results from multiple perspectives** even if one is primary

## Final Notes

- All sg_focus options evaluate the **same candidate subgroups**
  - The parameter only affects **which one is selected as primary**
  - No additional multiple testing burden between options
- Consider pre-specifying sg_focus in your statistical analysis plan

---
  
  *This document provides comprehensive guidance on the sg_focus parameter in ForestSearch. Each option represents a valid statistical and clinical perspective on subgroup identification for treatment harm.*

# Required R Packages for ForestSearch Analysis

1. **grf**
   - Generalized Random Forests for causal inference and subgroup identification.

2. **policytree**
   - Policy learning and tree-based subgroup identification.

3. **data.table**
   - Fast and memory-efficient data manipulation.

4. **randomForest**
   - Random forest algorithms for variable selection and prediction.

5. **survival**
   - Survival analysis, including Cox proportional hazards models.

6. **weightedSurv**
   - Weighted survival curves and related methods.

7. **future.apply**
   - Parallelization of apply functions using the future framework.

8. **foreach**
   - Looping construct for parallel execution.

9. **doFuture**
   - Backend for foreach to enable parallelization with future.

10. **doRNG**
    - Reproducible parallel foreach loops.

11. **DiagrammeR**
    - Visualization of diagrams and flowcharts (used for schematic diagrams).

## Usage Notes
- All packages should be installed before running the analysis.
- Some packages (e.g., `DiagrammeR`) are only needed for visualization and not for the core analysis.
- The function `ensure_packages()` in your code can be used to check and install missing packages automatically.


# Installing Required R Packages for ForestSearch Analysis

To run ForestSearch and its associated bootstrap/validation workflow, you need to install the following R packages:

- grf
- policytree
- data.table
- randomForest
- survival
- weightedSurv
- future.apply
- foreach
- doFuture
- doRNG
- DiagrammeR

You can install all these packages from CRAN using the following R code:

```r
# List of required packages
required_packages <- c(
  'grf', 'policytree', 'data.table', 'randomForest', 'survival',
  'future.apply', 'foreach', 'doFuture', 'doRNG', 'DiagrammeR'
)

# Install any packages that are not already installed
new_packages <- required_packages[!(required_packages %in% installed.packages()[,'Package'])]
if(length(new_packages)) install.packages(new_packages)

# Load all packages
lapply(required_packages, library, character.only = TRUE)

# For weightedSurv, at the time of this note, is only available on github

library(devtools)
install_github("larry-leon/weightedSurv")


```

**Notes:**
- You only need to run the installation code once per R environment.
- If you encounter any errors, make sure your R version is up to date and you have internet access.
- Some packages (e.g., `DiagrammeR`) are only needed for visualization and not for the core analysis.




# Summary of forestsearch_bootstrap_dofuture.R

This file provides functions for bootstrapping analysis in ForestSearch, with parallelization using doFuture.

## Key Functions

### ensure_packages
Installs and loads required packages if not already available.
- **Arguments:** pkgs (character vector of package names)
- **Returns:** None (loads packages)

### build_cox_formula
Constructs a Cox model formula from variable names.
- **Arguments:** outcome.name, event.name, treat.name (character)
- **Returns:** R formula object for Cox regression

### fit_cox_models
Fits Cox models for two subgroups defined by treatment recommendation.
- **Arguments:** df (data frame), formula (Cox model formula)
- **Returns:** List with HR and SE for each subgroup

### bootstrap_ystar
Generates a bootstrap matrix for Ystar using parallel processing.
- **Arguments:** df (data frame), nb_boots (integer)
- **Returns:** Matrix of bootstrap samples

### bootstrap_results
Runs bootstrap analysis for ForestSearch, fitting Cox models and bias correction.
- **Arguments:** fs.est (ForestSearch results), df_boot_analysis (data frame), cox.formula.boot (formula), nb_boots (integer), show_three (logical), H_obs, Hc_obs (numeric), reset_parallel (logical), boot_workers (integer)
- **Returns:** Data.table with bias-adjusted estimates and search metrics

### format_CI
Formats confidence interval for estimates.
- **Arguments:** estimates (data frame/data.table), col_names (character vector)
- **Returns:** Character string formatted as 'estimate (lower, upper)'

### forestsearch_bootstrap_dofuture
Orchestrates bootstrap analysis for ForestSearch using doFuture parallelization.
- **Arguments:** fs.est (ForestSearch results), nb_boots (integer), details (logical), show_three (logical), reset_parallel_fs (logical), boot_workers (integer), parallel_args (list)
- **Returns:** List with bootstrap results, confidence intervals, summary table, Ystar matrix, and estimates

## Usage Notes
- Ensure all required packages are installed before running bootstrap analysis.
- The main function for analysis is `forestsearch_bootstrap_dofuture`, which returns a comprehensive list of results for downstream analysis and reporting.
- Parallelization is handled via doFuture and foreach for efficient computation.


# Overview of Main functions


```{r flowchart2, fig.cap="Figure 2: Main functions", label="fig:flow2", echo = FALSE, fig.width = 10, fig.height = 10}
library(DiagrammeR)
main_flow <- grViz("
digraph main_flow {
  graph [rankdir = LR]
  node [shape=box, style=filled, fillcolor=\"#e6f2ff\", fontname=\"Arial\"]
  edge [fontname=\"Arial\"]
  forestsearch [label=\"forestsearch (main entry)\", fillcolor=\"#b3d1ff\"]
  get_FSdata [label=\"get_FSdata\"]
  grf_subg [label=\"grf.subg.harm.survival\"]
  subgroup_search [label=\"subgroup.search\"]
  subgroup_consistency [label=\"subgroup.consistency\"]
  get_dfpred [label=\"get_dfpred\"]
  SG_tab_estimates [label=\"SG_tab_estimates\"]
  forestsearch_bootstrap [label=\"forestsearch_bootstrap_dofuture\"]
  forestsearch -> get_FSdata
  forestsearch -> grf_subg
  forestsearch -> subgroup_search
  forestsearch -> subgroup_consistency
  forestsearch -> get_dfpred
  forestsearch -> SG_tab_estimates
  forestsearch -> forestsearch_bootstrap
}
", width = 600, height = 400)
main_flow
```


# Overview of Helper Functions


```{r flowchart3, fig.cap="Figure 3: Helpers functions", label="fig:flow3", echo = FALSE, fig.width = 10, fig.height = 10}
library(DiagrammeR)
helpers_flow <- grViz("
digraph helpers_flow {
  graph [rankdir = LR]
  node [shape=box, style=filled, fillcolor=\"#e6f2ff\", fontname=\"Arial\"]
  edge [fontname=\"Arial\"]
  # get_FSdata helpers
  get_FSdata [label=\"get_FSdata\"]
  get_FSdata -> lasso_selection
  get_FSdata -> get_conf_force
  get_FSdata -> filter_by_lassokeep
  get_FSdata -> is_continuous
  get_FSdata -> cut_var
  get_FSdata -> process_conf_force_expr
  get_FSdata -> dummy
  get_FSdata -> dummy2
  # grf_subg helpers
  grf_subg [label=\"grf.subg.harm.survival\"]
  grf_subg -> policy_tree
  grf_subg -> causal_survival_forest
  grf_subg -> double_robust_scores
  grf_subg -> aggregate
  # subgroup_search helpers
  subgroup_search [label=\"subgroup.search\"]
  subgroup_search -> get_combinations_info
  subgroup_search -> get_subgroup_membership
  subgroup_search -> get_covs_in
  subgroup_search -> extract_idx_flagredundancy
  # subgroup_consistency helpers
  subgroup_consistency [label=\"subgroup.consistency\"]
  subgroup_consistency -> sort_subgroups
  subgroup_consistency -> extract_subgroup
  subgroup_consistency -> sg_consistency_out
  subgroup_consistency -> remove_redundant_subgroups
  subgroup_consistency -> get_split_hr
  subgroup_consistency -> setup_parallel_SGcons
  # forestsearch_bootstrap helpers
  forestsearch_bootstrap [label=\"forestsearch_bootstrap_dofuture\"]
  forestsearch_bootstrap -> bootstrap_results
  forestsearch_bootstrap -> bootstrap_ystar
  forestsearch_bootstrap -> get_dfRes
  forestsearch_bootstrap -> get_Cox_sg
  forestsearch_bootstrap -> ci_est
  forestsearch_bootstrap -> get_targetEst
  forestsearch_bootstrap -> fit_cox_models
  forestsearch_bootstrap -> build_cox_formula
  forestsearch_bootstrap -> ensure_packages
  forestsearch_bootstrap -> setup_parallel_SGcons
  # SG_tab_estimates helpers
  SG_tab_estimates [label=\"SG_tab_estimates\"]
  SG_tab_estimates -> analyze_subgroup
  analyze_subgroup [label=\"analyze_subgroup\"]
  analyze_subgroup -> cox_summary
  analyze_subgroup -> km_summary
  analyze_subgroup -> rmst_calculation
  analyze_subgroup -> calculate_counts
  analyze_subgroup -> calculate_potential_hr
  SG_tab_estimates -> prepare_subgroup_data
  SG_tab_estimates -> format_results
  SG_tab_estimates -> format_CI
}
", width = 800, height = 800)
helpers_flow
```

# Overview of all functions

```{r flowchart4, fig.cap="Figure 4: Overview of ALL functions", label="fig:flow4", echo = FALSE, fig.width = 16, fig.height = 12}
library(DiagrammeR)

flowchart <- grViz("
digraph flowchart {
  graph [rankdir = LR]
  node [shape=box, style=filled, fillcolor=\"#e6f2ff\", fontname=\"Arial\"]
  edge [fontname=\"Arial\"]

  forestsearch [label=\"forestsearch (main entry)\", fillcolor=\"#b3d1ff\"]
  get_FSdata [label=\"get_FSdata\"]
  grf_subg [label=\"grf.subg.harm.survival\"]
  subgroup_search [label=\"subgroup.search\"]
  subgroup_consistency [label=\"subgroup.consistency\"]
  get_dfpred [label=\"get_dfpred\"]
  SG_tab_estimates [label=\"SG_tab_estimates\"]
  forestsearch_bootstrap [label=\"forestsearch_bootstrap_dofuture\"]

  # Main flow
  forestsearch -> get_FSdata
  forestsearch -> grf_subg
  forestsearch -> subgroup_search
  forestsearch -> subgroup_consistency
  forestsearch -> get_dfpred
  forestsearch -> SG_tab_estimates
  forestsearch -> forestsearch_bootstrap

  # get_FSdata helpers
  get_FSdata -> lasso_selection
  get_FSdata -> get_conf_force
  get_FSdata -> filter_by_lassokeep
  get_FSdata -> is_continuous
  get_FSdata -> cut_var
  get_FSdata -> process_conf_force_expr
  get_FSdata -> dummy
  get_FSdata -> dummy2

  # grf_subg helpers
  grf_subg -> policy_tree
  grf_subg -> causal_survival_forest
  grf_subg -> double_robust_scores
  grf_subg -> aggregate

  # subgroup_search helpers
  subgroup_search -> get_combinations_info
  subgroup_search -> get_subgroup_membership
  subgroup_search -> get_covs_in
  subgroup_search -> extract_idx_flagredundancy

  # subgroup_consistency helpers
  subgroup_consistency -> sort_subgroups
  subgroup_consistency -> extract_subgroup
  subgroup_consistency -> sg_consistency_out
  subgroup_consistency -> remove_redundant_subgroups
  subgroup_consistency -> get_split_hr
  subgroup_consistency -> setup_parallel_SGcons

  # forestsearch_bootstrap helpers
  forestsearch_bootstrap -> bootstrap_results
  forestsearch_bootstrap -> bootstrap_ystar
  forestsearch_bootstrap -> get_dfRes
  forestsearch_bootstrap -> get_Cox_sg
  forestsearch_bootstrap -> ci_est
  forestsearch_bootstrap -> get_targetEst
  forestsearch_bootstrap -> fit_cox_models
  forestsearch_bootstrap -> build_cox_formula
  forestsearch_bootstrap -> ensure_packages
  forestsearch_bootstrap -> setup_parallel_SGcons

  # SG_tab_estimates helpers
  SG_tab_estimates -> analyze_subgroup
  analyze_subgroup [label=\"analyze_subgroup\"]
  analyze_subgroup -> cox_summary
  analyze_subgroup -> km_summary
  analyze_subgroup -> rmst_calculation
  analyze_subgroup -> calculate_counts
  analyze_subgroup -> calculate_potential_hr
  SG_tab_estimates -> prepare_subgroup_data
  SG_tab_estimates -> format_results
  SG_tab_estimates -> format_CI
}
", width = 800, height = 800)
flowchart
```

